# 訊號的馴化：Tauri 架構下基於 ONNX 的降噪解決方案深度整合分析報告

## 1. 執行摘要：為決策者解構 ONNX 的戰略價值

在當前桌面應用程式開發的演進格局中，機器學習（Machine Learning, ML）功能的整合已從單純的技術亮點轉變為基礎性的功能需求。對於採用 Tauri 框架——一個以安全性和極致效能著稱的現代化應用程式架構——的開發團隊而言，引入 "ONNX" 技術堆疊不僅僅是增加一個函式庫，更是一次關鍵的架構轉型。

針對客戶對於 ONNX 僅聞其名而未解其實的現狀，本報告首章將深入剖析 ONNX（Open Neural Network Exchange，開放神經網路交換格式）的商業與技術雙重價值。

### 1.1 「通用轉接頭」與「AI 的 PDF」：ONNX 的核心概念

本質上，ONNX 扮演著人工智慧領域的 **互通性標準（Interoperability Standard）** 角色。在傳統的軟體開發生命週期中，**研究環境**（資料科學家使用 PyTorch 或 TensorFlow 訓練模型的場所）與 **生產環境**（軟體工程師將模型部署給使用者的終端）之間存在著巨大的技術鴻溝。若缺乏 ONNX，要部署一個 AI 模型，往往需要將龐大的訓練框架一同打包進應用程式中，這不僅會導致軟體體積臃腫，更會顯著降低執行效能。

我們可以將 ONNX 類比為「機器學習領域的通用轉接頭」或「AI 模型的 PDF」。正如 PDF 格式允許在 Microsoft Word 中建立的文件能夠在任何裝置上呈現一致的視覺效果，而無需安裝原始的文書處理軟體；ONNX 允許在任何主流框架中訓練出來的 AI 模型被「匯出」為一種標準化、高度最佳化的格式 1。這種格式是與框架無關的（Framework-Agnostic），這意味著生產環境的工程團隊無需管理研究團隊所使用的複雜依賴項，從而實現了模型研發與軟體工程的解耦。

進一步來說，ONNX 生態系統有效地將模型開發與硬體部署分離。訓練框架（如 PyTorch、TensorFlow）位於流程的上游，負責模型的創造；而標準化的 ONNX 格式位於中游，作為統一的交換媒介。這種架構設計使得下游的執行環境——即 ONNX Runtime（ORT）——能夠專注於在各種硬體後端（如 CPU、NVIDIA GPU、Intel OpenVINO 或 Apple CoreML）上高效地執行模型。這種流程設計確保了無論上游使用何種工具進行創新，下游的部署都能保持穩定且高效，無需為每個新模型重新構建推理引擎。

### 1.2 引擎的核心：ONNX Runtime (ORT)

如果說 ONNX 是檔案格式（藍圖），那麼 ONNX Runtime (ORT) 就是執行這張藍圖的引擎。對於 Tauri 應用程式而言，釐清這兩者的區別至關重要。ORT 是由微軟主導開發的高效能推理引擎，專門用於讀取 ONNX 檔案並利用硬體加速功能來執行運算 1。對於客戶而言，採用 ONNX Runtime 能為產品帶來三大決定性的商業與技術優勢：

1.  **硬體加速的不可知論（Hardware Acceleration Agnosticism）**： ORT 具備自動利用使用者機器上最佳可用硬體的能力。當使用者的電腦配備有 NVIDIA 顯示卡時，ORT 可以透過 CUDA 或 TensorRT 執行提供者（Execution Provider）來調用 GPU 算力；若使用的是 Intel 晶片，則可切換至 OpenVINO；在普通裝置上則回退至最佳化的 CPU 指令集（如 AVX2）。這確保了降噪功能既能在高階工作站上流暢運行，也能在電池供電的筆記型電腦上保持高效能，而無需開發人員為每種硬體編寫特定的程式碼 1。
2.  **跨平台的統一性**： 鑑於 Tauri 的目標是同時覆蓋 Windows、macOS 和 Linux，ORT 在這些作業系統上提供了統一的 API 介面。這意味著音訊處理的核心邏輯只需編寫一次，即可在所有平台上運行，極大地降低了維護特定平台程式碼的技術債 3。
3.  **未來的相容性與可擴展性**： 透過標準化 ONNX 格式，應用程式不會被鎖定在特定的模型架構上。假設下個季度學術界發布了一個效果更優的降噪模型（例如從 RNNoise 升級到 DeepFilterNet 3），工程團隊只需將舊的 .onnx 檔案替換為新檔案，只要輸入輸出的張量形狀（Tensor Shapes）保持相容，程式碼層面的變動將微乎其微 4。

## 2. ONNX 相容降噪模型的比較分析

選擇合適的降噪模型是一場在 **音訊品質**（透過 PESQ/MOS 分數衡量）、**運算成本**（以即時係數 Real-Time Factor, RTF 衡量） 以及 **延遲（Latency）** 之間博弈的平衡藝術。在即時通訊（VoIP）或會議轉錄的場景中，延遲是絕對的硬性約束；過高的延遲會導致「對講機效應」，即使用者在無意中互相搶話，嚴重破壞溝通體驗。本節將深入評估目前開源社群中最具潛力、且能透過 ONNX 整合至 Tauri 架構中的候選模型。

### 2.1 候選模型深度剖析

#### 2.1.1 DeepFilterNet (v2 & v3)

DeepFilterNet 是目前全頻帶語音增強領域的先進技術（State-of-Art, SOTA）代表。與傳統方法直接合成波形不同，它透過預測每個頻帶的增益（Gains/Filters）來處理音訊，這種「深層濾波（Deep Filtering）」的方法顯著提高了運算效率 5。

*   **架構解析**： 該模型採用了兩階段處理架構。第一階段利用卷積神經網路（CNN）與其編碼器-解碼器（Encoder-Decoder）結構來增強頻譜包絡（Spectral Envelope），即處理聲音的粗略結構；第二階段則利用循環神經網路（GRU/LSTM）來修復週期性結構，即聲音的細節部分 6。這種混合架構使其在處理非平穩噪音（如鍵盤敲擊聲、關門聲）方面表現卓越。
*   **效能表現**： 在感知品質上，DeepFilterNet 顯著優於 RNNoise 等早期模型。數據顯示，DeepFilterNet3 在多項客觀指標（如 PESQ, STOI）上均取得了領先成績，能有效保留語音的自然度，減少「機械音」的殘留。
*   **ONNX 相容性與挑戰**： 雖然該模型原生於 PyTorch 且支援導出為 ONNX，但在導出過程中需要特別注意複數運算（Complex-valued operations）的處理。此外，標準版模型的「前瞻（Look-ahead）」機制可能導致較高延遲。幸運的是，開發者社群已針對即時應用推出了「低延遲（Low Latency, LL）」變體，透過減少卷積層的前瞻緩衝區來換取更快的響應速度 7。
*   **延遲特性**： 標準版本的延遲可能超過 20ms，這在即時通訊中是可以察覺的。針對 Tauri 應用，必須選用 LL 變體以將演算法延遲控制在可接受範圍內（通常 < 10-15ms） 7。

#### 2.1.2 RNNoise

RNNoise 是輕量級降噪領域的老將，其核心理念是結合經典的數位訊號處理（DSP）與深度學習。它不依靠神經網路直接生成音訊，而是利用一個小型的循環神經網路（RNN）來計算語音活動機率和頻帶增益，最後由傳統的 DSP 濾波器執行實際的降噪操作。

*   **架構解析**： 混合 DSP/Deep Learning 架構。其參數量極低（約 60k 參數），這使得它在極端受限的硬體（如 Raspberry Pi 甚至更低階的嵌入式晶片）上也能流暢運行 9。
*   **效能表現**： 雖然運算成本極低，但其代價是音訊品質的妥協。以現代標準來看，RNNoise 的降噪效果僅屬「適中」。在處理複雜背景噪音時，它容易留下殘留噪音或產生「音樂雜訊（Musical Noise）」——即背景中類似流水或機器人的不自然聲音 9。
*   **ONNX 整合**： 雖然 RNNoise 原生是 C 語言實作，但現已有多個重寫版本支援導出為 ONNX。然而，由於其運算量本就極低，透過 ONNX Runtime 加速的邊際效益不如純深度學習模型明顯。

#### 2.1.3 Silero VAD (Voice Activity Detection)

嚴格來說，Silero VAD 並非降噪模型，而是語音活動檢測器。然而，在構建高效能降噪方案時，它是不可或缺的組件。它的作用是作為一道「閘門」，在檢測不到人聲時激進地將音訊靜音，從而徹底消除背景底噪。

*   **架構解析**： 基於輕量級深度神經網路，經過大量多語言數據訓練。
*   **效能表現**： 具有極致的速度（單個音訊區塊處理時間小於 1ms）和極高的準確率。它已成為業界公認的 VAD 標準 11。
*   **整合策略**： 在 Tauri 應用中，Silero VAD 通常與較重的降噪模型串聯使用。VAD 先行判斷，若無人聲則直接輸出靜音，跳過後續繁重的降噪運算，這能顯著節省 CPU 資源並提升使用者體驗（徹底的靜音比壓抑的噪音更舒適） 13。

#### 2.1.4 GTCRN (via Sherpa-ONNX)

GTCRN（Generalized Temporal Convolutional Recurrent Network）是近年來在 Sherpa-ONNX 框架下備受矚目的新星，代表了效率與品質的新平衡點。

*   **架構解析**： 這是一個複數值卷積循環網路。其設計極度精簡，參數數量僅約 0.05M（五萬個參數），遠低於傳統大模型 14。
*   **效率革命**： 數據顯示，GTCRN 的運算負載僅為 0.03 G/s MACs（每秒十億次乘加運算）。相比之下，較舊的高品質模型如 DCCRN 需要高達 14.36 G/s MACs。這意味著 GTCRN 的運算效率是 DCCRN 的數百倍，使其成為邊緣裝置（Edge Devices）和桌面應用背景執行的理想選擇 15。

### 2.2 模型效能與延遲的權衡分析

為了協助客戶做出戰略性決策，我們將上述模型的關鍵指標進行視覺化分析。在降噪系統的決策中，我們通常面臨「品質」、「速度（延遲）」與「運算成本」的三難困境。上圖展示了主要模型的性能分佈。我們可以觀察到 GTCRN 位於圖表的「甜蜜點」，它在保持極低運算複雜度（極小的氣泡）和低延遲的同時，提供了接近 DeepFilterNet 的高品質體驗。這使得 GTCRN 成為目前極具競爭力的選擇。

## 3. 架構戰略：Rust 後端 vs. WebView 前端

在 Tauri 架構中，決定即時音訊應用成敗的關鍵決策在於：**訊號處理究竟應該發生在哪裡？** 選擇主要在 **Rust Core（後端）** 還是 **WebView（前端/JavaScript）** 進行處理，將直接決定系統的穩定性與效能上限。

### 3.1 前端方案（WebView + ONNX Runtime Web）：便利的陷阱

在此方案中，應用程式透過瀏覽器的標準 API `navigator.mediaDevices.getUserMedia` 獲取音訊，並在 AudioWorklet 中使用 WebAssembly (WASM) 版本的 ONNX Runtime 進行處理。

*   **優勢**：
    *   **開發門檻低**： JavaScript/TypeScript 開發者資源豐富，生態系成熟。
    *   **移植性**： 程式碼本質上是一個運行在殼層中的 Web 應用，易於移植到純 Web 環境。
*   **劣勢（即時音訊的致命傷）**：
    *   **垃圾回收（Garbage Collection, GC）的不確定性**： JavaScript 是一種自動管理記憶體的語言。在即時音訊處理中，我們通常只有極短的時間預算（例如在 48kHz 取樣率、128 樣本緩衝區下，僅有約 2.6 毫秒）來處理一個幀。如果 JS 引擎的垃圾回收器（GC）恰好在此時介入進行記憶體清理，就會導致執行暫停。這種微小的暫停足以導致緩衝區欠載（Underflow），在使用者耳中表現為明顯的爆音、卡頓或雜訊（Clicks and Pops）16。
    *   **序列化開銷（Serialization Overhead）**： 雖然 Tauri 的前後端通訊很快，但如果需要將大量的音訊數據（Float32Array）在 Rust 後端（例如為了錄音存檔）與前端之間頻繁傳遞，其序列化與反序列化的開銷將會摧毀低延遲的預算 17。
    *   **執行緒限制**： 雖然 AudioWorklet 運行在獨立於 UI 的執行緒上，但它仍然受限於瀏覽器沙盒的資源調度，相比於原生執行緒，其優先順序與穩定性較難保證。

與前端方案不同，架構 B（Rust 後端處理）採取了截然不同的路徑。在此架構中，麥克風輸入的音訊數據直接由 Rust 的 `cpal` 函式庫在作業系統的高優先順序原生執行緒中截獲。這些數據隨即被送入同樣運行在原生環境下的 ONNX Runtime 進行推論處理，最後直接輸出至揚聲器。整個過程完全繞過了 WebView 和 IPC（行程間通訊）橋接器，這意味著音訊流完全不受 JavaScript 垃圾回收機制的干擾，也不會因為 UI 渲染的負載而產生波動。雖然這種架構犧牲了部分開發便利性，但它換取了對即時音訊至關重要的「確定性延遲」與系統穩定性。

### 3.2 後端方案（Rust + Native ONNX Runtime）：效能的堡壘

在此方案中，Rust 後端直接透過 `cpal` 等 Crates 管理音訊設備。它捕獲原始音訊緩衝區，將其傳遞給 `ort` Crate（ONNX Runtime 的 Rust 綁定）進行推理，並將純淨音訊寫入輸出設備或虛擬麥克風。前端僅作為控制面板（UI 按鈕、視覺化），接收非關鍵數據（如音量電平）進行顯示。

*   **優勢**：
    *   **確定性延遲（Deterministic Latency）**： Rust 允許手動管理記憶體。透過使用環形緩衝區（Ring Buffers）和預先分配的向量（Vectors），我們可以保證在「熱路徑（Hot Path）」（即音訊回調函數）中不發生任何動態記憶體分配，從而徹底消除 GC 暫停的風險 18。
    *   **執行緒安全**： Rust 獨有的所有權模型（Ownership Model）與 `Arc<Mutex>` 機制，確保了資料在音訊輸入執行緒、推理執行緒與輸出執行緒之間的安全共享，從編譯層面杜絕了數據競爭（Data Races）20。
    *   **極致效能**： 原生 ONNX Runtime 綁定（ort）直接調用底層 C++ 共享函式庫，繞過了 WASM 和瀏覽器沙盒的額外開銷。基準測試一致顯示，在繁重的計算任務中，Rust 實作的效能顯著優於 JS/WASM 21。
*   **劣勢**：
    *   **技術複雜度**： 需要深入理解 Rust 語言特性、執行緒同步機制以及音訊緩衝區的指針操作。
    *   **部署挑戰**： 需處理原生共享函式庫（如 onnxruntime.dll、libonnxruntime.so）在不同作業系統上的打包與路徑連結問題 23。

### 3.3 戰略判斷

**針對商業級的降噪應用，Rust 後端方案是唯一可行的選擇。** 前端方案由於 JavaScript 垃圾回收機制的非確定性，無法可靠地保證長時間運行下的無雜訊音訊體驗。因此，本報告後續將基於 Rust 後端架構進行深入的技術展開。

## 4. 技術實作戰略：構建「三執行緒管線」

在 Tauri 中使用 Rust 實作即時降噪系統，需要精心編排一系列函式庫（Crates）與執行緒模型。這不僅僅是程式碼的堆疊，更是對資料流動的嚴格管控。

### 4.1 Rust 音訊技術堆疊

為了構建前述的「後端架構」，我們需要以下關鍵組件：

*   **cpal (Cross-Platform Audio Library)**： 這是 Rust 生態中處理底層音訊 I/O 的標準。它負責管理從麥克風（輸入）和到揚聲器（輸出）的資料流回調（Callback）。
    *   **技術挑戰**： cpal 的回調函數是在極高優先順序的系統執行緒中執行的。開發者 **絕對不能** 在此回調中執行繁重的推理任務（如運行神經網路），因為這會阻塞音訊流，直接導致爆音 18。
    *   **解決方案**： 回調函數的唯一職責應該是將資料搬運到無鎖環形緩衝區中。
*   **ringbuf**： 一個無鎖（Lock-free）的單生產者-單消費者（SPSC）環形緩衝區是系統的心臟。它充當了嚴格即時要求的音訊執行緒與相對寬鬆的推理執行緒之間的「避震器」，確保資料流動的平滑 18。
*   **rubato**： 神經網路通常在特定的取樣率下運作（如 16kHz 或 48kHz），而使用者的麥克風設備可能設定為 44.1kHz 或 96kHz。rubato 提供了高品質的非同步重取樣功能，負責將硬體速率轉換為模型所需的速率 19。
*   **ort**： 這是 ONNX Runtime 的 Rust 封裝。它提供了 Session API 來載入模型，以及 run() 方法來執行推理。
    *   **關鍵配置**： 為了在 CPU 上獲得最佳效能，必須將其配置為 `ort::GraphOptimizationLevel::Level3` 以啟用最高級別的圖最佳化 28。

### 4.2 「三執行緒」設計模式 (The Triple-Thread Pattern)

為了確保系統的絕對穩定，我們建議採用嚴格分離的「三執行緒」設計模式。這是一個經過驗證的工程藍圖，旨在解決即時音訊處理中常見的競爭條件與阻塞問題。

在此架構中，資料流動呈現為一個單向、無阻塞的管線。

1.  **輸入執行緒（Input Thread）** 由 `cpal` 管理，它以極高的優先級運行，唯一的任務是將麥克風捕獲的原始音訊數據瞬間推入「輸入環形緩衝區」。這一步驟必須極快，不做任何處理。
2.  接著，資料進入 **處理執行緒（Processing Thread）**，這是由 Rust 應用程式生成的普通優先級執行緒。它從輸入緩衝區取出數據，首先透過 `rubato` 進行重取樣（例如從 44.1kHz 轉為 16kHz），接著將數據分批（Batching）成模型所需的區塊大小（例如 30ms 的幀），然後調用 `ort` 執行繁重的 ONNX 推理運算。運算完成後，將乾淨的音訊數據推入「輸出環形緩衝區」。
3.  最後，**輸出執行緒（Output Thread）** 同樣由 `cpal` 管理，具備高優先級。它從輸出緩衝區中拉取已處理好的數據，並將其寫入音訊硬體進行播放。

這種設計實現了完美的關注點分離（Separation of Concerns）：繁重的 AI 運算永遠不會阻塞敏感的音訊 I/O，從而從根本上杜絕了爆音與卡頓 20。

### 4.3 狀態管理：串流推理的關鍵 (Handling Model State)

大多數先進的降噪模型（如 DeepFilterNet, DCCRN, GTCRN）都是 **有狀態（Stateful）** 的循環神經網路。這意味著它們在處理當前一幀音訊時，必須依賴「上一幀的隱藏狀態（Hidden State）」來維持上下文資訊。

*   **ONNX 的複雜性**： 當這些有狀態模型被導出為 ONNX 時，原本封裝在模型內部的隱藏狀態通常會變成顯式的「輸入」與「輸出」。這意味著模型會有額外的輸入端口來接收舊狀態，並有額外的輸出端口來產生新狀態。
*   **實作細節**： Rust 程式碼必須手動管理這個狀態循環：初始化一個全零張量作為初始狀態。在第一次推理時，傳入 `Audio_Chunk + Zero_State`。模型輸出 `Clean_Audio + New_State`。Rust 程式碼必須暫存這個 `New_State`，並在處理下一個音訊區塊時，將其作為輸入傳回模型。這個「狀態迴圈」是模型能夠區分持續性噪音與語音的關鍵，若處理不當（例如每次都傳入零狀態），模型將失去記憶能力，降噪效果將大打折扣 30。

### 4.4 依賴管理與 DLL 部署 (The "Sidecar" Issue)

在 Tauri 與 `ort` 的整合中，最棘手的問題往往是跨平台分發。`ort` 依賴於底層的 C++ 動態連結庫（Shared Libraries），如 Windows 上的 `onnxruntime.dll`，macOS 上的 `libonnxruntime.dylib`，以及 Linux 上的 `libonnxruntime.so`。

*   **問題癥結**： 預設情況下，Rust 編譯出的執行檔是動態連結這些函式庫的。如果使用者的電腦系統路徑中沒有安裝 ONNX Runtime（這在一般使用者的電腦上是常態），應用程式一啟動就會因為找不到 DLL 而崩潰 24。
*   **解決戰略**：
    *   **啟用 copy-dylibs**： 在 `ort` crate 的 `Cargo.toml` 配置中啟用 `copy-dylibs` 功能。這會嘗試在編譯期間將必要的 DLL 自動複製到輸出目錄 31。
    *   **Tauri 資源打包 (Resource Bundling)**： 這是最穩健的做法。在 `tauri.conf.json` 的 `bundle > resources` 部分，明確指定將平台對應的共享函式庫包含在安裝包中。這確保了無論使用者是否安裝了開發環境，安裝包內都自帶了運行所需的依賴。
    *   **動態載入路徑配置**： 在 macOS 上，特別需要注意 `rpath` 或 `install_name_tool` 的問題。有時即便檔案存在，作業系統也可能因為路徑簽名問題拒絕載入。可能需要在應用程式啟動時，顯式告訴 `ort` 去哪裡尋找這些 bundled libraries，或者在構建腳本中使用 `install_name_tool` 修正 dylib 的載入路徑 23。

## 5. 基準測試與效能預期

為了有效管理客戶期望，我們必須基於真實數據來設定效能基準。以下數據綜合了上述模型在消費級硬體上透過 ONNX Runtime 運行的預期表現。

### 5.1 即時係數 (Real-Time Factor, RTF)

RTF 是衡量模型速度的核心指標，定義為 **處理時間 / 音訊長度**。RTF 為 0.1 意味著處理 10 秒的音訊只需要 1 秒。對於即時應用，RTF 必須小於 1.0，而在實際工程中，為了留給系統其他進程（如 UI 渲染、網路傳輸）足夠的資源，理想的 RTF 應小於 0.5。

*   **RNNoise**： 極致輕量。在現代 CPU 上 RTF 通常小於 0.05。對電池續航幾乎無影響，適合極端受限的設備 5。
*   **DeepFilterNet3**： RTF 範圍約在 0.15 到 0.35 之間（取決於 CPU 強度與是否使用 Int8 量化）。這雖然比 RNNoise 重，但對於現代桌面 CPU 來說仍然非常安全，完全在即時處理的能力範圍內 5。
*   **GTCRN**： 展現了驚人的效率。儘管品質極高，其 RTF 往往與 DeepFilterNet 持平甚至更優（接近 0.1 - 0.2），這歸功於其高度最佳化的卷積結構 15。

### 5.2 模型體積 (Deployment Footprint)

Tauri 的一大賣點是安裝包體積小。模型的選擇將直接影響最終安裝包的大小。

*   **RNNoise**： < 1 MB（幾乎可以忽略不計）。
*   **Silero VAD**： ~2 MB。
*   **DeepFilterNet**： ~5-8 MB（取決於是否進行壓縮）。
*   **ONNX Runtime Library**： ~10-20 MB。這是一個固定成本，無論使用哪個模型，都需要打包這個執行時函式庫。

### 5.3 延遲 (Latency)

這是指從使用者說話到乾淨音訊可用之間的時間差。

*   **DeepFilterNet (LL 變體)**： 演算法本身約 5-10ms 的前瞻延遲 + 處理時間。總體端到端延遲可控制在 20-30ms 左右，這對於 VoIP 是可接受的 32。
*   **RNNoise**： 幀大小帶來的延遲約 10ms + 極短的處理時間。
*   **系統開銷**： `cpal` 緩衝區與環形緩衝區本身會引入額外的 5-10ms 傳輸延遲。這是所有音訊應用都無法避免的物理極限。

## 6. 結論與戰略建議

綜合考量客戶對於在 Tauri 應用中整合降噪功能的需求，以及對效能與品質的雙重追求，我們提出以下具體的執行路線圖：

1.  **技術堆疊定案**： 堅定採用 **Rust 後端 (Rust Backend)** 架構。前端處理的風險過高，不適合商業產品。請使用 `cpal` 處理 I/O，`ringbuf` 確保執行緒安全，並使用 `ort` 進行高效的 ONNX 推理。
2.  **模型選擇策略**：
    *   **首選推薦**： **DeepFilterNet (Low Latency ONNX export)** 或 **GTCRN**。這兩者代表了目前的最佳平衡點。若追求極致的 SOTA 品質且硬體允許，DeepFilterNet 是穩妥之選；若希望在更低算力下獲得高品質，GTCRN 是極具潛力的挑戰者。
    *   **保底方案 (Lite Mode)**： **Silero VAD + Gain Gate**。如果發現某些低階用戶設備無法負荷上述模型，可以實作一個「輕量模式」，僅使用 VAD 檢測語音，在無人說話時自動靜音。這能以極低的成本解決 80% 的底噪問題。
3.  **部署關鍵**： 務必重視 DLL/dylib 的打包問題。利用 Tauri 的 `resources` 配置與 `ort` 的 `copy-dylibs` 功能，並在 CI/CD 流程（如 GitHub Actions）中進行跨平台構建測試，確保 Windows、macOS 和 Linux 用戶都能開箱即用。

透過此方案，我們不僅解決了客戶對 ONNX 的疑惑，更提供了一套經得起考驗的工程實作路徑，充分發揮 Tauri 架構「Rust 用於效能，Web 用於介面」的核心優勢，打造出專業級的音訊體驗。
